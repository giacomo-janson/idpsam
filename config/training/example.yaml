# Template configuration file for training a custom SAM model.
# The hyper-parameters of the models (AE and DDPM) are the sam of the idpSAM
# model.

# Generative model.
generative_model:
  data_type: cg_protein
  bead_type: ca
  encoding_dim: 16
  use_enc_std_scaler: true
  max_len: 60


# Training parameters.
training:
  out_dp: null
  # Data for training an autoencoder.
  ae:
    xyz_dp: null
    train: null
      proteins: null
      n_frames: 5000
      n_trajs: null
    val:
      proteins: null
      n_frames: 2500
      n_trajs: null
  # Data for training a generative model.
  gm:
    enc_dp: null
    train: null
      proteins: null
      n_frames: 5000
      n_trajs: null
    val:
      proteins: null
      n_frames: 2500
      n_trajs: null


# Encoder neural network.
encoder:
  arch: enc_ca_trf
  num_layers: 5
  embed_dim: 128
  embed_2d_dim: 192
  d_model: null
  num_heads: 8
  mlp_dim: 256
  dropout: null
  norm_eps: !!float 1e-5
  norm_pos: pre
  activation: gelu
  out_mode: idpgan_norm
  bead_embed_dim: 32
  pos_embed_dim: 64
  use_bias_2d: true
  pos_embed_r: 32
  embed_inject_mode: concat
  embed_2d_inject_mode: concat
  dmap_ca_min: 0.0
  dmap_ca_cutoff: 3.0
  dmap_ca_num_gaussians: 320
  dmap_embed_type: rbf
  use_dmap_mlp: true
  use_torsions: true
  weights: ./data/training/weights/nn.enc.pt
  std_scaler_fp: ./data/training/weights/enc_std_scaler.pt


# Decoder neural network.
decoder:
  type: deterministic
  arch: dec_ca_trf
  use_input_mlp: true
  num_layers: 5
  attention_type: timewarp
  embed_dim: 128
  d_model: 512
  num_heads: 32
  mlp_dim: 256
  dropout: null
  norm_eps: !!float 1e-5
  norm_pos: pre
  activation: gelu
  bead_embed_dim: null
  pos_embed_dim: 64
  use_bias_2d: true
  pos_embed_r: 32
  embed_inject_mode: null
  weights: ./data/training/weights/nn.dec.pt


# Generative model.
latent_generative_model:
  type: diffusers_dm
  sched_params:
    name: ddpm
    num_train_timesteps: 1000
    beta_start: 0.0001
    beta_end: 0.02
    beta_schedule: sigmoid
    variance_type: fixed_small
    prediction_type: epsilon
  loss: l2


# Neural network(s) used in the generative model.
latent_network:
  arch: eps_trf
  num_layers: 16
  attention_type: transformer
  embed_dim: 256
  d_model: null
  num_heads: 16
  mlp_dim: 512
  dropout: null
  norm_eps: !!float 1e-5
  norm_pos: pre
  activation: gelu
  out_mode: idpgan
  time_embed_dim: 256
  time_freq_dim: 256
  use_bead_embed: true
  bead_embed_dim: 32
  pt_embed_bead_dim: null
  pos_embed_dim: 64
  use_bias_2d: true
  pos_embed_r: 32
  edge_embed_dim: 192
  edge_embed_mode: null
  edge_update_mode: null
  edge_update_freq: 1
  embed_inject_mode: adanorm
  input_inject_mode: add
  input_inject_pos: out
  weights: ./data/training/weights/nn.eps.pt
